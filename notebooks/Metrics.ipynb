{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf14ef00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T16:04:44.383131Z",
     "start_time": "2022-06-01T16:04:42.371535Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import gensim\n",
    "import requests\n",
    "import html2text\n",
    "import wikipedia\n",
    "import smart_open\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTFeatureExtractor,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline,\n",
    ")\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"../../data\"\n",
    "\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5092f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T16:04:44.552993Z",
     "start_time": "2022-06-01T16:04:44.384564Z"
    }
   },
   "outputs": [],
   "source": [
    "root_path = os.path.join(data_path, \"okvqa\")\n",
    "\n",
    "test_questions_path = os.path.join(root_path, \"OpenEnded_mscoco_val2014_questions.json\")\n",
    "test_annotations_path = os.path.join(root_path, \"mscoco_val2014_annotations.json\")\n",
    "test_image_path = os.path.join(root_path, \"val2014\")\n",
    "test_image_name_prefix = \"COCO_val2014_000000\"\n",
    "\n",
    "with open(test_questions_path, \"r\") as f:\n",
    "    test_questions_df = pd.DataFrame(json.load(f)[\"questions\"])\n",
    "    \n",
    "with open(test_annotations_path, \"r\") as f:\n",
    "    test_annotations_df = pd.DataFrame(json.load(f)[\"annotations\"])\n",
    "    \n",
    "test_df = test_questions_df.merge(test_annotations_df)\n",
    "test_df[\"image_path\"] = test_df[\"image_id\"].map(lambda image_id: os.path.join(test_image_path, f\"{test_image_name_prefix}{image_id:06d}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4d8782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T16:05:06.717418Z",
     "start_time": "2022-06-01T16:04:44.555179Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "ic_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "ic_feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "ic_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_model = qa_model.to(device)\n",
    "ic_model = ic_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63b99ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T16:06:41.591989Z",
     "start_time": "2022-06-01T16:06:41.577890Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "        i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    return i_image\n",
    "\n",
    "def get_wikipedia_page(string):\n",
    "    try:\n",
    "        p = wikipedia.summary(string, auto_suggest=False)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        s = e.options[0]\n",
    "        p = wikipedia.summary(s, auto_suggest=False)\n",
    "\n",
    "    return p\n",
    "\n",
    "def get_context_wikipedia(caption):\n",
    "    words = word_tokenize(caption)\n",
    "    tags = pos_tag(words)\n",
    "\n",
    "    tags = [w for (w, t) in tags if t[0] == \"N\"]\n",
    "\n",
    "    options = list(set([o for tag in tags for o in wikipedia.search(tag, results=1)]))\n",
    "    pages = [caption] + [get_wikipedia_page(option) for option in options]\n",
    "\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def get_caption(\n",
    "    ic_model,\n",
    "    ic_feature_extractor,\n",
    "    ic_tokenizer,\n",
    "    image,\n",
    "    max_length=16,\n",
    "    num_beams=4,\n",
    "):\n",
    "    pixel_values = ic_feature_extractor(\n",
    "        images=[image], return_tensors=\"pt\"\n",
    "    ).pixel_values\n",
    "    pixel_values = pixel_values.to(ic_model.device)\n",
    "\n",
    "    output_ids = ic_model.generate(\n",
    "        pixel_values, max_length=max_length, num_beams=num_beams\n",
    "    )\n",
    "\n",
    "    preds = ic_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds[0]\n",
    "\n",
    "def get_answer(qa_model, qa_tokenizer, question, context):\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)\n",
    "    result = qa_pipeline(question, context)\n",
    "\n",
    "    return result\n",
    "\n",
    "def run(\n",
    "    ic_feature_extractor,\n",
    "    ic_model,\n",
    "    ic_tokenizer,\n",
    "    qa_model,\n",
    "    qa_tokenizer,\n",
    "    image,\n",
    "    question,\n",
    "    max_length=16,\n",
    "    num_beams=4,\n",
    "):\n",
    "    caption = get_caption(\n",
    "        ic_model,\n",
    "        ic_feature_extractor,\n",
    "        ic_tokenizer,\n",
    "        image,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    context = get_context_wikipedia(caption)\n",
    "    answer = get_answer(qa_model, qa_tokenizer, question, context)\n",
    "\n",
    "    return caption, answer, context\n",
    "\n",
    "def mk_predictions(index):\n",
    "    question, image_path, answers = test_df[[\"question\", \"image_path\", \"answers\"]].iloc[index]\n",
    "    image = read_image(image_path)\n",
    "    caption, answer, context = run(ic_feature_extractor, ic_model, ic_tokenizer, qa_model, \n",
    "                                   qa_tokenizer, image, question)\n",
    "    \n",
    "    return answer, answers\n",
    "\n",
    "def get_similarity(doc2vec, pair):\n",
    "    answer, answers = pair\n",
    "    \n",
    "    result = 0\n",
    "    for ans in list(set([a['answer'] for a in answers])):\n",
    "        r = doc2vec.similarity_unseen_docs(word_tokenize(answer[\"answer\"]), word_tokenize(ans))\n",
    "        if r > result:\n",
    "            result = r\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_score(doc2vec, predictions, threshold=0.5):\n",
    "    total = len(predictions)\n",
    "    count = 0\n",
    "    \n",
    "    for pair in predictions:\n",
    "        if get_similarity(doc2vec, pair) >= threshold:\n",
    "            count += 1\n",
    "            \n",
    "    return count / total\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e91b06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T16:07:41.049101Z",
     "start_time": "2022-06-01T16:07:39.330368Z"
    }
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file)) + list(read_corpus(lee_test_file))\n",
    "\n",
    "doc2vec = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "doc2vec.build_vocab(train_corpus)\n",
    "doc2vec.train(train_corpus, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49f95c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T18:24:03.048759Z",
     "start_time": "2022-06-01T16:36:55.680152Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 1000/1000 [1:47:07<00:00,  6.43s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    predictions.append(mk_predictions(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b63a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T18:24:09.893415Z",
     "start_time": "2022-06-01T18:24:03.050675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.307"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score(doc2vec, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b823fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
